{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1ac2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a8c529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'word', 'meaning1', 'meaning2', 'meaning3', 'meaning4',\n",
       "       'meaning5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv(\"wordle_dataset.csv\")\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99dbde26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097890b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>meaning1</th>\n",
       "      <th>meaning2</th>\n",
       "      <th>meaning3</th>\n",
       "      <th>meaning4</th>\n",
       "      <th>meaning5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aachen</td>\n",
       "      <td>a city in western germany near the dutch and b...</td>\n",
       "      <td>formerly it was charlemagnes northern capital</td>\n",
       "      <td>aixlachapelle</td>\n",
       "      <td>aachen</td>\n",
       "      <td>aken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>nocturnal burrowing mammal of the grasslands o...</td>\n",
       "      <td>sole extant representative of the order tubuli...</td>\n",
       "      <td>anteater</td>\n",
       "      <td>ant bear</td>\n",
       "      <td>orycteropus afer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaron</td>\n",
       "      <td>united states professional baseball player who...</td>\n",
       "      <td>old testament elder brother of moses and first...</td>\n",
       "      <td>henry louis aaron</td>\n",
       "      <td>aaron</td>\n",
       "      <td>hank aaron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aas</td>\n",
       "      <td>an associate degree in applied science</td>\n",
       "      <td>a dry form of lava resembling clinkers an inte...</td>\n",
       "      <td>associate in arts</td>\n",
       "      <td>aa</td>\n",
       "      <td>associate in applied science alcoholics anonym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abaca</td>\n",
       "      <td>a kind of hemp obtained from the abaca plant i...</td>\n",
       "      <td>philippine banana tree having leafstalks that ...</td>\n",
       "      <td>manila hemp</td>\n",
       "      <td>manilla hemp</td>\n",
       "      <td>musa textilis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word                                           meaning1  \\\n",
       "0    aachen  a city in western germany near the dutch and b...   \n",
       "1  aardvark  nocturnal burrowing mammal of the grasslands o...   \n",
       "2     aaron  united states professional baseball player who...   \n",
       "3       aas             an associate degree in applied science   \n",
       "4     abaca  a kind of hemp obtained from the abaca plant i...   \n",
       "\n",
       "                                            meaning2           meaning3  \\\n",
       "0      formerly it was charlemagnes northern capital      aixlachapelle   \n",
       "1  sole extant representative of the order tubuli...           anteater   \n",
       "2  old testament elder brother of moses and first...  henry louis aaron   \n",
       "3  a dry form of lava resembling clinkers an inte...  associate in arts   \n",
       "4  philippine banana tree having leafstalks that ...        manila hemp   \n",
       "\n",
       "       meaning4                                           meaning5  \n",
       "0        aachen                                               aken  \n",
       "1      ant bear                                   orycteropus afer  \n",
       "2         aaron                                         hank aaron  \n",
       "3            aa  associate in applied science alcoholics anonym...  \n",
       "4  manilla hemp                                      musa textilis  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f000bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21440"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b97fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to read from a binary file\n",
    "import pickle\n",
    "file=open(\"bert_embeddings.bin\",\"rb\")\n",
    "f=pickle.load(file)\n",
    "#print(f)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e70dd6",
   "metadata": {},
   "source": [
    "print(f['embedding1'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c7be1e",
   "metadata": {},
   "source": [
    "len(f['embedding1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a81ba74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding1</th>\n",
       "      <th>embedding2</th>\n",
       "      <th>embedding3</th>\n",
       "      <th>embedding4</th>\n",
       "      <th>embedding5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.10552966, -0.2111916, -0.042547125, 0.13038...</td>\n",
       "      <td>[-0.7410908, 0.3037946, 1.0067592, -0.03756384...</td>\n",
       "      <td>[-0.70134115, -0.1560492, 1.8670809, 0.0931303...</td>\n",
       "      <td>[-0.42886475, 0.06789703, 2.4393466, 0.1563847...</td>\n",
       "      <td>[-0.14427099, 0.28605372, 1.3342835, 0.1262898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.15250994, 0.59140694, -0.34323326, 0.12957...</td>\n",
       "      <td>[-0.04384717, 0.6120334, 0.790435, 0.06521284,...</td>\n",
       "      <td>[0.009730791, 0.496809, 0.41013932, 0.1373665,...</td>\n",
       "      <td>[0.12894955, 1.4916507, -0.31830812, -0.232148...</td>\n",
       "      <td>[0.09932873, -0.13440955, 1.4096322, 0.3468186...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.17691979, -0.27761042, -0.4785753, 0.32844...</td>\n",
       "      <td>[0.42689332, 1.4958502, -0.3949241, -0.3236406...</td>\n",
       "      <td>[-0.17629763, 0.86110705, 0.8352665, 0.2713195...</td>\n",
       "      <td>[-0.28148162, 0.32501546, 1.6964121, 0.1526436...</td>\n",
       "      <td>[-0.36929166, 1.0149219, 1.2935079, 0.23599564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.1955431, 0.28204358, 1.6014135, 0.2948678,...</td>\n",
       "      <td>[-0.5539297, 1.3073884, 0.76646256, 0.10328976...</td>\n",
       "      <td>[-0.20551097, 0.21470872, 1.7353882, 0.3388543...</td>\n",
       "      <td>[0.21038896, 0.10880705, 2.3038127, 0.29001892...</td>\n",
       "      <td>[-0.21374436, 1.316712, 0.68170404, -0.1837967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.08495724, -1.0941188, 0.16890968, 0.784468...</td>\n",
       "      <td>[-0.24078937, -0.09390615, -0.29544133, 0.4669...</td>\n",
       "      <td>[0.09469901, -0.7468935, 0.6064788, 0.5366701,...</td>\n",
       "      <td>[0.012085825, -1.0377014, 1.0099944, 0.3805281...</td>\n",
       "      <td>[-0.24604101, -0.117436886, 1.4289687, -0.1175...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          embedding1  \\\n",
       "0  [0.10552966, -0.2111916, -0.042547125, 0.13038...   \n",
       "1  [-0.15250994, 0.59140694, -0.34323326, 0.12957...   \n",
       "2  [-0.17691979, -0.27761042, -0.4785753, 0.32844...   \n",
       "3  [-0.1955431, 0.28204358, 1.6014135, 0.2948678,...   \n",
       "4  [-0.08495724, -1.0941188, 0.16890968, 0.784468...   \n",
       "\n",
       "                                          embedding2  \\\n",
       "0  [-0.7410908, 0.3037946, 1.0067592, -0.03756384...   \n",
       "1  [-0.04384717, 0.6120334, 0.790435, 0.06521284,...   \n",
       "2  [0.42689332, 1.4958502, -0.3949241, -0.3236406...   \n",
       "3  [-0.5539297, 1.3073884, 0.76646256, 0.10328976...   \n",
       "4  [-0.24078937, -0.09390615, -0.29544133, 0.4669...   \n",
       "\n",
       "                                          embedding3  \\\n",
       "0  [-0.70134115, -0.1560492, 1.8670809, 0.0931303...   \n",
       "1  [0.009730791, 0.496809, 0.41013932, 0.1373665,...   \n",
       "2  [-0.17629763, 0.86110705, 0.8352665, 0.2713195...   \n",
       "3  [-0.20551097, 0.21470872, 1.7353882, 0.3388543...   \n",
       "4  [0.09469901, -0.7468935, 0.6064788, 0.5366701,...   \n",
       "\n",
       "                                          embedding4  \\\n",
       "0  [-0.42886475, 0.06789703, 2.4393466, 0.1563847...   \n",
       "1  [0.12894955, 1.4916507, -0.31830812, -0.232148...   \n",
       "2  [-0.28148162, 0.32501546, 1.6964121, 0.1526436...   \n",
       "3  [0.21038896, 0.10880705, 2.3038127, 0.29001892...   \n",
       "4  [0.012085825, -1.0377014, 1.0099944, 0.3805281...   \n",
       "\n",
       "                                          embedding5  \n",
       "0  [-0.14427099, 0.28605372, 1.3342835, 0.1262898...  \n",
       "1  [0.09932873, -0.13440955, 1.4096322, 0.3468186...  \n",
       "2  [-0.36929166, 1.0149219, 1.2935079, 0.23599564...  \n",
       "3  [-0.21374436, 1.316712, 0.68170404, -0.1837967...  \n",
       "4  [-0.24604101, -0.117436886, 1.4289687, -0.1175...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(f)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f256a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d030678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>meaning1</th>\n",
       "      <th>meaning2</th>\n",
       "      <th>meaning3</th>\n",
       "      <th>meaning4</th>\n",
       "      <th>meaning5</th>\n",
       "      <th>embedding1</th>\n",
       "      <th>embedding2</th>\n",
       "      <th>embedding3</th>\n",
       "      <th>embedding4</th>\n",
       "      <th>embedding5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaron</td>\n",
       "      <td>united states professional baseball player who...</td>\n",
       "      <td>old testament elder brother of moses and first...</td>\n",
       "      <td>henry louis aaron</td>\n",
       "      <td>aaron</td>\n",
       "      <td>hank aaron</td>\n",
       "      <td>[-0.17691979, -0.27761042, -0.4785753, 0.32844...</td>\n",
       "      <td>[0.42689332, 1.4958502, -0.3949241, -0.3236406...</td>\n",
       "      <td>[-0.17629763, 0.86110705, 0.8352665, 0.2713195...</td>\n",
       "      <td>[-0.28148162, 0.32501546, 1.6964121, 0.1526436...</td>\n",
       "      <td>[-0.36929166, 1.0149219, 1.2935079, 0.23599564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abaca</td>\n",
       "      <td>a kind of hemp obtained from the abaca plant i...</td>\n",
       "      <td>philippine banana tree having leafstalks that ...</td>\n",
       "      <td>manila hemp</td>\n",
       "      <td>manilla hemp</td>\n",
       "      <td>musa textilis</td>\n",
       "      <td>[-0.08495724, -1.0941188, 0.16890968, 0.784468...</td>\n",
       "      <td>[-0.24078937, -0.09390615, -0.29544133, 0.4669...</td>\n",
       "      <td>[0.09469901, -0.7468935, 0.6064788, 0.5366701,...</td>\n",
       "      <td>[0.012085825, -1.0377014, 1.0099944, 0.3805281...</td>\n",
       "      <td>[-0.24604101, -0.117436886, 1.4289687, -0.1175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abase</td>\n",
       "      <td>cause to feel shame</td>\n",
       "      <td>hurt the pride of</td>\n",
       "      <td>humble</td>\n",
       "      <td>mortify</td>\n",
       "      <td>chagrin humiliate</td>\n",
       "      <td>[-0.06568777, -0.018044014, 1.3753768, 0.29626...</td>\n",
       "      <td>[0.07058689, -0.023161145, 1.4258491, 0.644862...</td>\n",
       "      <td>[-0.03826209, -0.22021341, 1.4970418, 0.335306...</td>\n",
       "      <td>[0.50851375, 0.36827028, 2.4313066, 0.3968379,...</td>\n",
       "      <td>[0.020256568, 0.23802361, 1.9627985, 0.5381185...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abate</td>\n",
       "      <td>make less active or intense</td>\n",
       "      <td>become less in amount or intensity</td>\n",
       "      <td>let up</td>\n",
       "      <td>slack off</td>\n",
       "      <td>die away slake slack</td>\n",
       "      <td>[0.30710104, -0.19726573, 1.3976676, 0.1641309...</td>\n",
       "      <td>[0.18145579, -0.26938137, 1.3242697, 0.3711263...</td>\n",
       "      <td>[0.17421165, -0.14604211, 2.3641486, 0.2870952...</td>\n",
       "      <td>[0.17828284, -0.14433196, 2.1683278, 0.2864399...</td>\n",
       "      <td>[0.14779839, 0.17784989, 1.7109579, 0.3073285,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abele</td>\n",
       "      <td>a poplar that is widely cultivated in the unit...</td>\n",
       "      <td>has white bark and leaves with whitish undersu...</td>\n",
       "      <td>white aspen</td>\n",
       "      <td>white poplar</td>\n",
       "      <td>silverleaved poplar populus alba aspen poplar</td>\n",
       "      <td>[-0.28837916, -0.18204403, 0.3476545, -0.10453...</td>\n",
       "      <td>[0.086071245, -0.24445783, -0.64354473, 0.1595...</td>\n",
       "      <td>[-0.6212785, 0.024624296, -0.50231385, 0.31722...</td>\n",
       "      <td>[-0.22014526, -0.23819354, -0.33803833, 0.1581...</td>\n",
       "      <td>[-0.10784195, 0.50255656, -0.11357013, 0.08043...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word                                           meaning1  \\\n",
       "0  aaron  united states professional baseball player who...   \n",
       "1  abaca  a kind of hemp obtained from the abaca plant i...   \n",
       "2  abase                                cause to feel shame   \n",
       "3  abate                        make less active or intense   \n",
       "4  abele  a poplar that is widely cultivated in the unit...   \n",
       "\n",
       "                                            meaning2           meaning3  \\\n",
       "0  old testament elder brother of moses and first...  henry louis aaron   \n",
       "1  philippine banana tree having leafstalks that ...        manila hemp   \n",
       "2                                  hurt the pride of             humble   \n",
       "3                 become less in amount or intensity             let up   \n",
       "4  has white bark and leaves with whitish undersu...        white aspen   \n",
       "\n",
       "       meaning4                                       meaning5  \\\n",
       "0         aaron                                     hank aaron   \n",
       "1  manilla hemp                                  musa textilis   \n",
       "2       mortify                              chagrin humiliate   \n",
       "3     slack off                           die away slake slack   \n",
       "4  white poplar  silverleaved poplar populus alba aspen poplar   \n",
       "\n",
       "                                          embedding1  \\\n",
       "0  [-0.17691979, -0.27761042, -0.4785753, 0.32844...   \n",
       "1  [-0.08495724, -1.0941188, 0.16890968, 0.784468...   \n",
       "2  [-0.06568777, -0.018044014, 1.3753768, 0.29626...   \n",
       "3  [0.30710104, -0.19726573, 1.3976676, 0.1641309...   \n",
       "4  [-0.28837916, -0.18204403, 0.3476545, -0.10453...   \n",
       "\n",
       "                                          embedding2  \\\n",
       "0  [0.42689332, 1.4958502, -0.3949241, -0.3236406...   \n",
       "1  [-0.24078937, -0.09390615, -0.29544133, 0.4669...   \n",
       "2  [0.07058689, -0.023161145, 1.4258491, 0.644862...   \n",
       "3  [0.18145579, -0.26938137, 1.3242697, 0.3711263...   \n",
       "4  [0.086071245, -0.24445783, -0.64354473, 0.1595...   \n",
       "\n",
       "                                          embedding3  \\\n",
       "0  [-0.17629763, 0.86110705, 0.8352665, 0.2713195...   \n",
       "1  [0.09469901, -0.7468935, 0.6064788, 0.5366701,...   \n",
       "2  [-0.03826209, -0.22021341, 1.4970418, 0.335306...   \n",
       "3  [0.17421165, -0.14604211, 2.3641486, 0.2870952...   \n",
       "4  [-0.6212785, 0.024624296, -0.50231385, 0.31722...   \n",
       "\n",
       "                                          embedding4  \\\n",
       "0  [-0.28148162, 0.32501546, 1.6964121, 0.1526436...   \n",
       "1  [0.012085825, -1.0377014, 1.0099944, 0.3805281...   \n",
       "2  [0.50851375, 0.36827028, 2.4313066, 0.3968379,...   \n",
       "3  [0.17828284, -0.14433196, 2.1683278, 0.2864399...   \n",
       "4  [-0.22014526, -0.23819354, -0.33803833, 0.1581...   \n",
       "\n",
       "                                          embedding5  \n",
       "0  [-0.36929166, 1.0149219, 1.2935079, 0.23599564...  \n",
       "1  [-0.24604101, -0.117436886, 1.4289687, -0.1175...  \n",
       "2  [0.020256568, 0.23802361, 1.9627985, 0.5381185...  \n",
       "3  [0.14779839, 0.17784989, 1.7109579, 0.3073285,...  \n",
       "4  [-0.10784195, 0.50255656, -0.11357013, 0.08043...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only keep those rows where the length of the word is 5\n",
    "words_data2 = words_data[words_data['word'].str.len() == 5] \n",
    "words_data2 = words_data2.reset_index(drop=True)\n",
    "words_data2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e4fdf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2174"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64933937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random word from the dataset\n",
    "def choose_random_word(df):\n",
    "    index = random.randint(0,len(df))\n",
    "    print(\"index : \",index)\n",
    "    print(\"word chosen : \",words_data2[\"word\"][index])\n",
    "    return df[\"word\"][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ec68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate score based on the number of tries\n",
    "def calculate_score(tries):\n",
    "    return 15 - (tries - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca7b61",
   "metadata": {},
   "source": [
    "### HINT 1: SEMANTICALLY SIMILAR WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7061e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3bf0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4610d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6d43b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity_column3(query_vec,df):\n",
    "    l1 = []\n",
    "    match3 = {}\n",
    "    for index,sent in enumerate(df['embedding3']):\n",
    "        sim = cosine(query_vec, sent)\n",
    "\n",
    "        if sim>0.75:\n",
    "            match3[index]=sim\n",
    "            \n",
    "    print(len(match3))\n",
    "\n",
    "    s_match3 = sorted(match3.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_10_keys3 = [item[0] for item in s_match3[:10]]\n",
    "    print(top_10_keys3)\n",
    "    \n",
    "    for i in top_10_keys3:\n",
    "        print(df['word'][i])\n",
    "        l1.append(df['word'][i])\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1f96c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity_column4(query_vec,df):\n",
    "    l2 = []\n",
    "    match4 = {}\n",
    "    for index,sent in enumerate(df['embedding4']):\n",
    "        sim = cosine(query_vec, sent)\n",
    "        #print(\"Sent = \", sent, \"; similarity = \", sim)\n",
    "\n",
    "        if sim>0.75:\n",
    "            match4[index]=sim\n",
    "\n",
    "    print(len(match4))\n",
    "\n",
    "    s_match4 = sorted(match4.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_10_keys4 = [item[0] for item in s_match4[:10]]\n",
    "    print(top_10_keys4)\n",
    "    \n",
    "    for i in top_10_keys4:\n",
    "        print(df['word'][i])\n",
    "        l2.append(df['word'][i])\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1b4235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity_column5(query_vec,df):\n",
    "    l3 = []\n",
    "    match5 = {}\n",
    "    for index,sent in enumerate(df['embedding5']):\n",
    "        sim = cosine(query_vec, sent)\n",
    "        #print(\"Sent = \", sent, \"; similarity = \", sim)\n",
    "\n",
    "        if sim>0.75:\n",
    "            match5[index]=sim\n",
    "    print(len(match5))\n",
    "\n",
    "    s_match5 = sorted(match5.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_10_keys5 = [item[0] for item in s_match5[:10]]\n",
    "    print(top_10_keys5)\n",
    "    \n",
    "    for i in top_10_keys5:\n",
    "        print(df['word'][i])\n",
    "        l3.append(df['word'][i])\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b0032be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Find a word semantically similar to the chosen word\n",
    "def find_similar_word(query, df):\n",
    "    query_vec = model1.encode([query])[0]\n",
    "    l1 = semantic_similarity_column3(query_vec,words_data)\n",
    "    l2 = semantic_similarity_column4(query_vec,words_data)\n",
    "    l3 = semantic_similarity_column5(query_vec,words_data)\n",
    "    \n",
    "    final_list = l1+l2+l3\n",
    "    final_list = sorted(list(set(final_list)))\n",
    "    \n",
    "    if query in final_list:\n",
    "        final_list.remove(query)\n",
    "    print(final_list)\n",
    "    \n",
    "    length = len(final_list)\n",
    "    list_for_hint = []\n",
    "    for i in range(5):\n",
    "        random_index = random.randint(0,length-1)\n",
    "        if final_list[random_index] not in list_for_hint:\n",
    "            list_for_hint.append(final_list[random_index])\n",
    "    print(list_for_hint)\n",
    "    return list_for_hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34a2c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 2: Keyword extraction from the meanings\n",
    "def extract_keywords(meanings):\n",
    "    # Combine meanings into a single string\n",
    "    text = ' '.join(meanings)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    # Calculate frequency distribution of words\n",
    "    word_freq = Counter(filtered_tokens)\n",
    "    # Return most common keywords\n",
    "    return [word for word, _ in word_freq.most_common(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7e04ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordle_game(words_data,words_data2):\n",
    "    chosen_word = choose_random_word(words_data2)\n",
    "    tries = 1\n",
    "    score = 15\n",
    "    hint_used = {'semantic': False, 'keyword': False}\n",
    "\n",
    "    print(\"Welcome to Wordle! You have 7 tries to guess the word.\")\n",
    "\n",
    "    while tries <= 7:\n",
    "        guess = input(f\"\\nAttempt {tries}: Enter your guess (or type 'give up' to give up): \").lower()\n",
    "\n",
    "        if guess == 'give up':\n",
    "            print(f\"Sorry, you've given up. The word was '{chosen_word}'!\")\n",
    "            score -= 8\n",
    "            break\n",
    "\n",
    "        if guess == chosen_word.lower():\n",
    "            print(f\"Congratulations! You've guessed the word '{chosen_word}' correctly!\")\n",
    "            score += 10\n",
    "            break\n",
    "\n",
    "        score -= 2\n",
    "\n",
    "        correct_letters = sum(1 for x, y in zip(guess, chosen_word) if x == y)\n",
    "        misplaced_letters = sum(min(guess.count(letter), chosen_word.count(letter)) for letter in set(guess)) - correct_letters\n",
    "\n",
    "        correct_letters = 0\n",
    "        misplaced_letters = 0\n",
    "        correct_positions = []\n",
    "        correct_letters_wrong_position = []\n",
    "\n",
    "        # Check for correct letters in correct positions and correct letters in wrong positions\n",
    "        for idx, letter in enumerate(guess):\n",
    "            if letter == chosen_word[idx]:\n",
    "                correct_letters += 1\n",
    "                correct_positions.append(letter)\n",
    "            elif letter in chosen_word:\n",
    "                misplaced_letters += 1\n",
    "                correct_letters_wrong_position.append(letter)\n",
    "\n",
    "        print(f\"Correct letters in correct positions: {correct_positions}\")\n",
    "        print(f\"Correct letters in wrong positions: {correct_letters_wrong_position}\")\n",
    "        print(f\"Your current score: {score}\")\n",
    "        \n",
    "        '''print(f\"Correct letters in correct positions: {correct_letters}\")\n",
    "        print(f\"Correct letters in wrong positions: {misplaced_letters}\")\n",
    "        print(f\"Your current score: {score}\")'''\n",
    "\n",
    "        tries += 1\n",
    "\n",
    "        if tries == 2:\n",
    "            hint_choice = input(\"Do you want to use a hint? (yes/no): \").lower()\n",
    "            if hint_choice == 'yes':\n",
    "                if not hint_used['semantic']:\n",
    "                    hint_used['semantic'] = True\n",
    "                    similar_word = find_similar_word(chosen_word, words_data)\n",
    "                    print(f\"\\nHint 1: A word similar to the chosen word is '{similar_word}'.\")\n",
    "                else:\n",
    "                    print(\"You have already used the semantic similarity hint.\")\n",
    "            else:\n",
    "                print(\"No hint used.\")\n",
    "\n",
    "        elif tries > 2:\n",
    "            hint_choice = input(\"Do you want to use a hint? (yes/no): \").lower()\n",
    "            if hint_choice == 'yes':\n",
    "                if not hint_used['keyword']:\n",
    "                    hint_used['keyword'] = True\n",
    "                    print(\"Hint 2: Keywords extracted from the meanings are:\", extract_keywords([chosen_word['meaning1'], chosen_word['meaning2']]))\n",
    "                else:\n",
    "                    print(\"You have already used both hints.\")\n",
    "            else:\n",
    "                print(\"No hint used.\")\n",
    "\n",
    "    if tries > 7:\n",
    "        print(f\"\\nSorry, you've run out of tries! The word was '{chosen_word}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bb287ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index :  1922\n",
      "word chosen :  thorn\n",
      "Welcome to Wordle! You have 7 tries to guess the word.\n",
      "\n",
      "Attempt 1: Enter your guess (or type 'give up' to give up): throb\n",
      "Correct letters in correct positions: ['t', 'h']\n",
      "Correct letters in wrong positions: ['r', 'o']\n",
      "Your current score: 13\n",
      "Do you want to use a hint? (yes/no): yes\n",
      "1870\n",
      "[14508, 14510, 19003, 3039, 3049, 4236, 6616, 8808, 8815, 11606]\n",
      "pricker\n",
      "prickle\n",
      "thorn\n",
      "center\n",
      "centre\n",
      "core\n",
      "essence\n",
      "heart\n",
      "hearts\n",
      "marrow\n",
      "1841\n",
      "[2596, 17696, 181, 1957, 10281, 18768, 14507, 10142, 10512, 12822]\n",
      "burred\n",
      "spiny\n",
      "acerbity\n",
      "bitterness\n",
      "jaundice\n",
      "tartness\n",
      "prick\n",
      "inwardness\n",
      "kernel\n",
      "nub\n",
      "231\n",
      "[4006, 20482, 14817, 8974, 10887, 6769, 20233, 4453, 11713, 18184]\n",
      "conscience\n",
      "vellicate\n",
      "punishable\n",
      "hind\n",
      "leipoa\n",
      "excruciate\n",
      "unswept\n",
      "crepe\n",
      "maze\n",
      "strive\n",
      "['acerbity', 'bitterness', 'burred', 'center', 'centre', 'conscience', 'core', 'crepe', 'essence', 'excruciate', 'heart', 'hearts', 'hind', 'inwardness', 'jaundice', 'kernel', 'leipoa', 'marrow', 'maze', 'nub', 'prick', 'pricker', 'prickle', 'punishable', 'spiny', 'strive', 'tartness', 'unswept', 'vellicate']\n",
      "['prick', 'hearts', 'crepe', 'heart', 'core']\n",
      "\n",
      "Hint 1: A word similar to the chosen word is '['prick', 'hearts', 'crepe', 'heart', 'core']'.\n",
      "\n",
      "Attempt 2: Enter your guess (or type 'give up' to give up): roses\n",
      "Correct letters in correct positions: []\n",
      "Correct letters in wrong positions: ['r', 'o']\n",
      "Your current score: 11\n",
      "Do you want to use a hint? (yes/no): no\n",
      "No hint used.\n",
      "\n",
      "Attempt 3: Enter your guess (or type 'give up' to give up): gie up\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwordle_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwords_data2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 34\u001b[0m, in \u001b[0;36mwordle_game\u001b[1;34m(words_data, words_data2)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Check for correct letters in correct positions and correct letters in wrong positions\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, letter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(guess):\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m letter \u001b[38;5;241m==\u001b[39m \u001b[43mchosen_word\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m     35\u001b[0m         correct_letters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     36\u001b[0m         correct_positions\u001b[38;5;241m.\u001b[39mappend(letter)\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "wordle_game(words_data,words_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ffc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652070f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9cb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7cc06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9d92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce042938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
