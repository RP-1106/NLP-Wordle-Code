{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134637d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3deedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab2ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"rl-sentence-compression/data/train-data/gigaword/train.jsonl\"\n",
    "val_path = \"rl-sentence-compression/data/train-data/gigaword/val.jsonl\"\n",
    "test_path = \"rl-sentence-compression/data/test-data/gigaword.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "164ad0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(train_path)\n",
    "val_data = load_data(val_path)\n",
    "test_data = load_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47a3b071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "189651\n",
      "1951\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbad4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[0:4000]\n",
    "val_data   = val_data[0:4000]\n",
    "test_data = test_data[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd64221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "4000\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0c4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_id(data):\n",
    "    updated_data = []\n",
    "    for item in data:\n",
    "        updated_item = {key:value for key,value in item.items() if key != \"id\"}\n",
    "        updated_data.append(updated_item)\n",
    "    return updated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c1822c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = remove_id(train_data)\n",
    "test_data = remove_id(test_data)\n",
    "val_data = remove_id(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ca07eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data)\n",
    "val_df   = pd.DataFrame(val_data)\n",
    "test_df  = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c5d366e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>australia 's current account deficit shrunk by...</td>\n",
       "      <td>australian current account deficit narrows sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>at least two people were killed in a suspected...</td>\n",
       "      <td>at least two dead in southern philippines blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>australian shares closed down #.# percent mond...</td>\n",
       "      <td>australian stocks close down #.# percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>south korea 's nuclear envoy kim sook urged no...</td>\n",
       "      <td>envoy urges north korea to restart nuclear dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>south korea on monday announced sweeping tax r...</td>\n",
       "      <td>skorea announces tax cuts to stimulate economy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  australia 's current account deficit shrunk by...   \n",
       "1  at least two people were killed in a suspected...   \n",
       "2  australian shares closed down #.# percent mond...   \n",
       "3  south korea 's nuclear envoy kim sook urged no...   \n",
       "4  south korea on monday announced sweeping tax r...   \n",
       "\n",
       "                                             summary  \n",
       "0  australian current account deficit narrows sha...  \n",
       "1    at least two dead in southern philippines blast  \n",
       "2           australian stocks close down #.# percent  \n",
       "3  envoy urges north korea to restart nuclear dis...  \n",
       "4     skorea announces tax cuts to stimulate economy  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90809536",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.rename(columns = {'summaries':'summary'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5371c679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>japan 's nec corp. and UNK computer corp. of t...</td>\n",
       "      <td>nec UNK in computer sales tie-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the sri lankan government on wednesday announc...</td>\n",
       "      <td>sri lanka closes schools as war escalates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  japan 's nec corp. and UNK computer corp. of t...   \n",
       "1  the sri lankan government on wednesday announc...   \n",
       "\n",
       "                                     summary  \n",
       "0           nec UNK in computer sales tie-up  \n",
       "1  sri lanka closes schools as war escalates  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(test_df['text'][0]))\n",
    "print(type(test_df['summary'][0]))\n",
    "test_df['summary'] = test_df['summary'].str[0].astype(str)\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b6ab44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_integer(value):\n",
    "    try:\n",
    "        int(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "994bba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train_df.iterrows():\n",
    "    if any(is_integer(value) for value in row):\n",
    "        train_df.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18f3aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in val_df.iterrows():\n",
    "    if any(is_integer(value) for value in row):\n",
    "        val_df.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f547add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_df.iterrows():\n",
    "    if any(is_integer(value) for value in row):\n",
    "        test_df.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16173a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5695f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.head(2000)\n",
    "val_df   = val_df.head(2000)\n",
    "test_df  = test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b69558e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rhea Pandita\\AppData\\Local\\Temp\\ipykernel_16828\\3653395451.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_df[column] = train_df[column].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    }
   ],
   "source": [
    "for col in train_df.columns:\n",
    "    train_df[col] = train_df[col].str.lower()\n",
    "for column in train_df.columns:\n",
    "    train_df[column] = train_df[column].str.lstrip().str.rstrip()\n",
    "for column in train_df.columns:\n",
    "    train_df[column] = train_df[column].str.replace(r'[^\\w\\s]+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "255ddc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sentence length (word count)\n",
    "max_len_word = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3af2f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rhea Pandita\\AppData\\Local\\Temp\\ipykernel_16828\\3456752920.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  val_df[column] = val_df[column].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    }
   ],
   "source": [
    "for col in val_df.columns:\n",
    "    val_df[col] = val_df[col].str.lower()\n",
    "for column in val_df.columns:\n",
    "    val_df[column] = val_df[column].str.lstrip().str.rstrip()\n",
    "for column in val_df.columns:\n",
    "    val_df[column] = val_df[column].str.replace(r'[^\\w\\s]+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8396f69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rhea Pandita\\AppData\\Local\\Temp\\ipykernel_16828\\836522378.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_df[column] = test_df[column].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    }
   ],
   "source": [
    "for col in test_df.columns:\n",
    "    test_df[col] = test_df[col].str.lower()\n",
    "for column in test_df.columns:\n",
    "    test_df[column] = test_df[column].str.lstrip().str.rstrip()\n",
    "for column in test_df.columns:\n",
    "    test_df[column] = test_df[column].str.replace(r'[^\\w\\s]+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02b13490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7199982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text and summary data\n",
    "def tokenize_text(text, max_length):\n",
    "    print(len(text))\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = text.tolist()\n",
    "    #print(text)\n",
    "    tokens = tokenizer(text, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors='tf')\n",
    "    #print(tokens)\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d176919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text data\n",
    "text_train_input, text_train_mask = tokenize_text(train_df['text'], max_len_word)\n",
    "text_val_input, text_val_mask = tokenize_text(val_df['text'], max_len_word)\n",
    "text_test_input, text_test_mask = tokenize_text(test_df['text'], max_len_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29908e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 150)\n",
      "(2000, 150)\n",
      "(3, 150)\n"
     ]
    }
   ],
   "source": [
    "print(text_train_mask.shape)\n",
    "print(text_val_mask.shape)\n",
    "print(text_test_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d71e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reshape mask tensors\n",
    "text_train_mask = np.expand_dims(text_train_mask, axis=-1)\n",
    "text_val_mask = np.expand_dims(text_val_mask, axis=-1)\n",
    "text_test_mask = np.expand_dims(text_test_mask, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4cbd6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 150, 1)\n",
      "(2000, 150, 1)\n",
      "(3, 150, 1)\n"
     ]
    }
   ],
   "source": [
    "print(text_train_mask.shape)\n",
    "print(text_val_mask.shape)\n",
    "print(text_test_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35e68eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(150,), dtype=int32, numpy=\n",
       "array([  101,  2660,  1055,  2783,  4070, 15074, 14021, 15532,  2243,\n",
       "        2011,  1037,  2501,  4551,  6363,  1048, 15185,  4551,  2149,\n",
       "       25269,  2497,  1999,  1996,  2238,  4284,  2349,  2000, 23990,\n",
       "       19502,  7597,  4481,  2207,  6928,  3662,   102,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0])>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train_input[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821f462",
   "metadata": {},
   "source": [
    "# Load Sentence Transformer model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7cd16d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "104c9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sentences using BERT\n",
    "def encode_sentences(sentences):\n",
    "    encoding = tokenizer.batch_encode_plus(sentences,\n",
    "                                           padding=True,\n",
    "                                           truncation=True,\n",
    "                                           return_tensors='pt',\n",
    "                                           add_special_tokens=True)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        word_embeddings = outputs.last_hidden_state \n",
    "    return word_embeddings,input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4534679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text data\n",
    "text_embeddings_train_bert,train_ids = encode_sentences(train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f9be1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings_val_bert,val_ids = encode_sentences(val_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "423ce9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings_test_bert,test_ids = encode_sentences(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c250021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 54, 768])\n",
      "2000\n",
      "torch.Size([2000, 56, 768])\n",
      "2000\n",
      "torch.Size([3, 33, 768])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(text_embeddings_train_bert.shape)\n",
    "print(len(text_embeddings_train_bert))\n",
    "\n",
    "print(text_embeddings_val_bert.shape)\n",
    "print(len(text_embeddings_val_bert))\n",
    "\n",
    "print(text_embeddings_test_bert.shape)\n",
    "print(len(text_embeddings_test_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4deab2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 150, 768)\n",
      "(2000, 150, 768)\n",
      "(3, 150, 768)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Padding length\n",
    "max_sequence_length = 150\n",
    "\n",
    "# Pad the sequences to ensure consistent length\n",
    "text_embeddings_train_bert = pad_sequences(text_embeddings_train_bert, maxlen=max_sequence_length, padding='post')\n",
    "text_embeddings_val_bert = pad_sequences(text_embeddings_val_bert, maxlen=max_sequence_length, padding='post')\n",
    "text_embeddings_test_bert = pad_sequences(text_embeddings_test_bert, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Now check the shape of the padded sequences\n",
    "print(text_embeddings_train_bert.shape)\n",
    "print(text_embeddings_val_bert.shape)\n",
    "print(text_embeddings_test_bert.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ae8830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, LayerNormalization\n",
    "from tensorflow.keras.layers import Reshape, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f430f10",
   "metadata": {},
   "source": [
    "# Modify model architecture to accept BERT-encoded inputs\n",
    "input_text_bert = Input(shape=(text_embeddings_train_bert.shape[1], text_embeddings_train_bert.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b6f21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (150, 768)  # Assuming BERT model output size is 768\n",
    "\n",
    "# Define input layer\n",
    "input_text = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f85ca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 150, 768)\n"
     ]
    }
   ],
   "source": [
    "print(input_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd4e7c",
   "metadata": {},
   "source": [
    "text_embeddings_bert = input_text_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8187516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2abdb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "num_bert_layers = 12\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f551a",
   "metadata": {},
   "source": [
    "for _ in range(num_bert_layers):\n",
    "    text_embeddings_bert = Dropout(dropout_rate)(text_embeddings_bert)\n",
    "    for _ in range(3):\n",
    "        text_embeddings_bert = Dense(text_embeddings_bert.shape[-1], activation=\"relu\")(text_embeddings_bert)\n",
    "        text_embeddings_bert = Dropout(dropout_rate)(text_embeddings_bert)\n",
    "        text_embeddings_bert = Dense(text_embeddings_bert.shape[-1])(text_embeddings_bert)\n",
    "        \n",
    "        # Add skip connection\n",
    "        text_embeddings_bert = text_embeddings_bert + input_text_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0f9e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_bert_layers):\n",
    "    text_embeddings = Dropout(dropout_rate)(text_embeddings)\n",
    "    for _ in range(3):\n",
    "        text_embeddings = Dense(text_embeddings.shape[-1], activation=\"relu\")(text_embeddings)\n",
    "        text_embeddings = Dropout(dropout_rate)(text_embeddings)\n",
    "        text_embeddings = Dense(text_embeddings.shape[-1])(text_embeddings)\n",
    "        \n",
    "        # Add skip connection\n",
    "        text_embeddings = text_embeddings + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f215a82",
   "metadata": {},
   "source": [
    "text_output_bert = Dense(text_train_mask.shape[-1], activation=\"sigmoid\")(text_embeddings_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a90bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the output to match the target shape\n",
    "from tensorflow.keras.layers import Reshape, Lambda, TimeDistributed\n",
    "text_output = TimeDistributed(Dense(1, activation=\"sigmoid\"))(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "881af0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# Remove the extra dimension from the output tensor\n",
    "text_output = Flatten()(text_output)\n",
    "\n",
    "# Final feedforward layer with sigmoid activation\n",
    "text_output = Dense(150, activation=\"sigmoid\")(text_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8992b637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 150)\n"
     ]
    }
   ],
   "source": [
    "print(text_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69820d",
   "metadata": {},
   "source": [
    "model_bert = Model(inputs=input_text_bert, outputs=text_output_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6185a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model_bert = Model(inputs=input_text, outputs=text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2e427b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=1e-5, epsilon=1e-8)\n",
    "model_bert.compile(optimizer=optimizer, loss=binary_crossentropy, \n",
    "                   metrics=[binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "416553db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1352s\u001b[0m 20s/step - binary_accuracy: 0.5455 - loss: 0.6930 - val_binary_accuracy: 0.5805 - val_loss: 0.6788\n",
      "Epoch 2/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1890s\u001b[0m 29s/step - binary_accuracy: 0.6024 - loss: 0.6731 - val_binary_accuracy: 0.5908 - val_loss: 0.6652\n",
      "Epoch 3/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1433s\u001b[0m 21s/step - binary_accuracy: 0.6118 - loss: 0.6584 - val_binary_accuracy: 0.6340 - val_loss: 0.6455\n",
      "Epoch 4/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1429s\u001b[0m 23s/step - binary_accuracy: 0.6456 - loss: 0.6383 - val_binary_accuracy: 0.6631 - val_loss: 0.6223\n",
      "Epoch 5/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1159s\u001b[0m 18s/step - binary_accuracy: 0.6727 - loss: 0.6123 - val_binary_accuracy: 0.7073 - val_loss: 0.5837\n",
      "Epoch 6/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1147s\u001b[0m 18s/step - binary_accuracy: 0.7229 - loss: 0.5678 - val_binary_accuracy: 0.7435 - val_loss: 0.5447\n",
      "Epoch 7/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1054s\u001b[0m 16s/step - binary_accuracy: 0.7567 - loss: 0.5301 - val_binary_accuracy: 0.7746 - val_loss: 0.5126\n",
      "Epoch 8/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1661s\u001b[0m 23s/step - binary_accuracy: 0.7870 - loss: 0.4990 - val_binary_accuracy: 0.8017 - val_loss: 0.4847\n",
      "Epoch 9/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1125s\u001b[0m 17s/step - binary_accuracy: 0.8137 - loss: 0.4711 - val_binary_accuracy: 0.8255 - val_loss: 0.4598\n",
      "Epoch 10/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1536s\u001b[0m 24s/step - binary_accuracy: 0.8375 - loss: 0.4468 - val_binary_accuracy: 0.8435 - val_loss: 0.4372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fababe7eb0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with BERT-encoded inputs\n",
    "model_bert.fit(text_embeddings_train_bert, text_train_mask, \n",
    "               validation_data=(text_embeddings_val_bert, text_val_mask), \n",
    "               epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6422090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries for test data using the trained model\n",
    "text_predicted_summary_bert = model_bert.predict(text_embeddings_test_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0860d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated summaries\n",
    "def decode_summary_bert1(embeddings, test_ids, tokenizer):\n",
    "    decoded_summaries = []\n",
    "    for i in range(len(embeddings)):\n",
    "        summary_text = tokenizer.decode(test_ids[i], skip_special_tokens=True)\n",
    "        decoded_summaries.append(summary_text)\n",
    "    return decoded_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95f9582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'tokenizer' is your BERT tokenizer instance\n",
    "decoded_summaries_bert = decode_summary_bert1(text_predicted_summary_bert, \n",
    "                                             test_ids,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "840bb063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 Summary: japan s nec corp and unk computer corp of the united states said wednesday they had agreed to join forces in supercomputer sales Len: 128\n",
      "\n",
      "Sample 2 Summary: the sri lankan government on wednesday announced the closure of government schools with immediate effect as a military campaign against tamil separatists escalated in the north of the country Len: 191\n",
      "\n",
      "Sample 3 Summary: police arrested five antinuclear protesters thursday after they sought to disrupt loading of a french antarctic research and supply vessel a spokesman for the protesters said Len: 174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print summary text for each sample\n",
    "for i, summary_text in enumerate(decoded_summaries_bert):\n",
    "    print(f\"Sample {i+1} Summary: {summary_text} Len: {len(summary_text)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "504d42ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated summaries\n",
    "def decode_summary_bert2(embeddings, tokenizer):\n",
    "    decoded_summaries = []\n",
    "    for i in range(len(embeddings)):\n",
    "        summary_text = tokenizer.decode(test_ids[i], skip_special_tokens=True)\n",
    "        decoded_summaries.append(summary_text)\n",
    "    return decoded_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc624d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'tokenizer' is your BERT tokenizer instance\n",
    "decoded_summaries_bert = decode_summary_bert2(text_predicted_summary_bert, \n",
    "                                             tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3f1f44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 Summary: japan s nec corp and unk computer corp of the united states said wednesday they had agreed to join forces in supercomputer sales Len: 128\n",
      "\n",
      "Sample 2 Summary: the sri lankan government on wednesday announced the closure of government schools with immediate effect as a military campaign against tamil separatists escalated in the north of the country Len: 191\n",
      "\n",
      "Sample 3 Summary: police arrested five antinuclear protesters thursday after they sought to disrupt loading of a french antarctic research and supply vessel a spokesman for the protesters said Len: 174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print summary text for each sample\n",
    "for i, summary_text in enumerate(decoded_summaries_bert):\n",
    "    print(f\"Sample {i+1} Summary: {summary_text} Len: {len(summary_text)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text_predicted_summary_bert:\n",
    "    a = tokenizer.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d4013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2da158c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0923592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load BERT tokenizer\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "997b997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Rhea is there\",\"Water is there\",\"Fire is there\"]\n",
    "encoding = tokenizer.batch_encode_plus( text,\n",
    "                                      padding=True,\n",
    "                                      truncation=True,\n",
    "                                      return_tensors='pt',\n",
    "                                      add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30a162f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 24775,  2003,  2045,   102],\n",
      "        [  101,  2300,  2003,  2045,   102],\n",
      "        [  101,  2543,  2003,  2045,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c7f3ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rhea', 'is', 'there']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e525ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_text = []\n",
    "for i in text:\n",
    "    marked_text.append(\"[CLS] \"+i+\" [SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83d2432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] Rhea is there [SEP]', '[CLS] Water is there [SEP]', '[CLS] Fire is there [SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(marked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cba17b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = []\n",
    "indexed_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7952d7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'rhea', 'is', 'there', '[SEP]']\n",
      "[101, 24775, 2003, 2045, 102]\n",
      "\n",
      "['[CLS]', 'water', 'is', 'there', '[SEP]']\n",
      "[101, 2300, 2003, 2045, 102]\n",
      "\n",
      "['[CLS]', 'fire', 'is', 'there', '[SEP]']\n",
      "[101, 2543, 2003, 2045, 102]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in marked_text:\n",
    "    tokenized_text = tokenizer.tokenize(i)\n",
    "    print(tokenized_text)\n",
    "    indexed_token  = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    print(indexed_token)\n",
    "    print()\n",
    "    tokenized_texts.append(tokenized_text)\n",
    "    indexed_tokens.append(indexed_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bda961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', 'rhea', 'is', 'there', '[SEP]'], ['[CLS]', 'water', 'is', 'there', '[SEP]'], ['[CLS]', 'fire', 'is', 'there', '[SEP]']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "256b0b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 24775, 2003, 2045, 102], [101, 2300, 2003, 2045, 102], [101, 2543, 2003, 2045, 102]]\n"
     ]
    }
   ],
   "source": [
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9e41ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "rhea         24,775\n",
      "is            2,003\n",
      "there         2,045\n",
      "[SEP]           102\n",
      "[CLS]           101\n",
      "water         2,300\n",
      "is            2,003\n",
      "there         2,045\n",
      "[SEP]           102\n",
      "[CLS]           101\n",
      "fire          2,543\n",
      "is            2,003\n",
      "there         2,045\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(tokenized_texts, indexed_tokens):\n",
    "    for k,l in zip(i,j):\n",
    "        print('{:<12} {:>6,}'.format(k,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60a12805",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73205174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 768])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    word_embeddings = outputs.last_hidden_state \n",
    "    print(word_embeddings.shape)\n",
    "    print(len(word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e84baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_summary =[]\n",
    "for i in range(len(input_ids)):\n",
    "    decoded_text = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "    decoded_text = tokenizer.tokenize(decoded_text)\n",
    "    decoded_summary.append(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3f78b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rhea', 'is', 'there']\n",
      "['water', 'is', 'there']\n",
      "['fire', 'is', 'there']\n"
     ]
    }
   ],
   "source": [
    "for i in decoded_summary:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26499088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token :  fire\n",
      "Token :  is\n",
      "Token :  there\n"
     ]
    }
   ],
   "source": [
    "for token,i in zip(decoded_text,word_embeddings[0]):\n",
    "    print(\"Token : \",token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f6f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
