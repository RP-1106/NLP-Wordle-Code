{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115bfdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fcf22b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'word', 'meaning1', 'meaning2', 'meaning3', 'meaning4',\n",
       "       'meaning5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv(\"wordle_dataset.csv\")\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6ee1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(['Unnamed: 0'],axis=1)\n",
    "df1 = df1.dropna()\n",
    "df1 = df1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e236f4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>meaning1</th>\n",
       "      <th>meaning2</th>\n",
       "      <th>meaning3</th>\n",
       "      <th>meaning4</th>\n",
       "      <th>meaning5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aachen</td>\n",
       "      <td>a city in western germany near the dutch and b...</td>\n",
       "      <td>formerly it was charlemagnes northern capital</td>\n",
       "      <td>aixlachapelle</td>\n",
       "      <td>aachen</td>\n",
       "      <td>aken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>nocturnal burrowing mammal of the grasslands o...</td>\n",
       "      <td>sole extant representative of the order tubuli...</td>\n",
       "      <td>anteater</td>\n",
       "      <td>ant bear</td>\n",
       "      <td>orycteropus afer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaron</td>\n",
       "      <td>united states professional baseball player who...</td>\n",
       "      <td>old testament elder brother of moses and first...</td>\n",
       "      <td>henry louis aaron</td>\n",
       "      <td>aaron</td>\n",
       "      <td>hank aaron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aas</td>\n",
       "      <td>an associate degree in applied science</td>\n",
       "      <td>a dry form of lava resembling clinkers an inte...</td>\n",
       "      <td>associate in arts</td>\n",
       "      <td>aa</td>\n",
       "      <td>associate in applied science alcoholics anonym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abaca</td>\n",
       "      <td>a kind of hemp obtained from the abaca plant i...</td>\n",
       "      <td>philippine banana tree having leafstalks that ...</td>\n",
       "      <td>manila hemp</td>\n",
       "      <td>manilla hemp</td>\n",
       "      <td>musa textilis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word                                           meaning1  \\\n",
       "0    aachen  a city in western germany near the dutch and b...   \n",
       "1  aardvark  nocturnal burrowing mammal of the grasslands o...   \n",
       "2     aaron  united states professional baseball player who...   \n",
       "3       aas             an associate degree in applied science   \n",
       "4     abaca  a kind of hemp obtained from the abaca plant i...   \n",
       "\n",
       "                                            meaning2           meaning3  \\\n",
       "0      formerly it was charlemagnes northern capital      aixlachapelle   \n",
       "1  sole extant representative of the order tubuli...           anteater   \n",
       "2  old testament elder brother of moses and first...  henry louis aaron   \n",
       "3  a dry form of lava resembling clinkers an inte...  associate in arts   \n",
       "4  philippine banana tree having leafstalks that ...        manila hemp   \n",
       "\n",
       "       meaning4                                           meaning5  \n",
       "0        aachen                                               aken  \n",
       "1      ant bear                                   orycteropus afer  \n",
       "2         aaron                                         hank aaron  \n",
       "3            aa  associate in applied science alcoholics anonym...  \n",
       "4  manilla hemp                                      musa textilis  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb7bf011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21435"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99dc7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to read from a binary file\n",
    "import pickle\n",
    "file=open(\"bert_embeddings.bin\",\"rb\")\n",
    "f=pickle.load(file)\n",
    "#print(f)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7e4663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding1</th>\n",
       "      <th>embedding2</th>\n",
       "      <th>embedding3</th>\n",
       "      <th>embedding4</th>\n",
       "      <th>embedding5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.10552966, -0.2111916, -0.042547125, 0.13038...</td>\n",
       "      <td>[-0.7410908, 0.3037946, 1.0067592, -0.03756384...</td>\n",
       "      <td>[-0.70134115, -0.1560492, 1.8670809, 0.0931303...</td>\n",
       "      <td>[-0.42886475, 0.06789703, 2.4393466, 0.1563847...</td>\n",
       "      <td>[-0.14427099, 0.28605372, 1.3342835, 0.1262898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.15250994, 0.59140694, -0.34323326, 0.12957...</td>\n",
       "      <td>[-0.04384717, 0.6120334, 0.790435, 0.06521284,...</td>\n",
       "      <td>[0.009730791, 0.496809, 0.41013932, 0.1373665,...</td>\n",
       "      <td>[0.12894955, 1.4916507, -0.31830812, -0.232148...</td>\n",
       "      <td>[0.09932873, -0.13440955, 1.4096322, 0.3468186...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.17691979, -0.27761042, -0.4785753, 0.32844...</td>\n",
       "      <td>[0.42689332, 1.4958502, -0.3949241, -0.3236406...</td>\n",
       "      <td>[-0.17629763, 0.86110705, 0.8352665, 0.2713195...</td>\n",
       "      <td>[-0.28148162, 0.32501546, 1.6964121, 0.1526436...</td>\n",
       "      <td>[-0.36929166, 1.0149219, 1.2935079, 0.23599564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.1955431, 0.28204358, 1.6014135, 0.2948678,...</td>\n",
       "      <td>[-0.5539297, 1.3073884, 0.76646256, 0.10328976...</td>\n",
       "      <td>[-0.20551097, 0.21470872, 1.7353882, 0.3388543...</td>\n",
       "      <td>[0.21038896, 0.10880705, 2.3038127, 0.29001892...</td>\n",
       "      <td>[-0.21374436, 1.316712, 0.68170404, -0.1837967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.08495724, -1.0941188, 0.16890968, 0.784468...</td>\n",
       "      <td>[-0.24078937, -0.09390615, -0.29544133, 0.4669...</td>\n",
       "      <td>[0.09469901, -0.7468935, 0.6064788, 0.5366701,...</td>\n",
       "      <td>[0.012085825, -1.0377014, 1.0099944, 0.3805281...</td>\n",
       "      <td>[-0.24604101, -0.117436886, 1.4289687, -0.1175...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          embedding1  \\\n",
       "0  [0.10552966, -0.2111916, -0.042547125, 0.13038...   \n",
       "1  [-0.15250994, 0.59140694, -0.34323326, 0.12957...   \n",
       "2  [-0.17691979, -0.27761042, -0.4785753, 0.32844...   \n",
       "3  [-0.1955431, 0.28204358, 1.6014135, 0.2948678,...   \n",
       "4  [-0.08495724, -1.0941188, 0.16890968, 0.784468...   \n",
       "\n",
       "                                          embedding2  \\\n",
       "0  [-0.7410908, 0.3037946, 1.0067592, -0.03756384...   \n",
       "1  [-0.04384717, 0.6120334, 0.790435, 0.06521284,...   \n",
       "2  [0.42689332, 1.4958502, -0.3949241, -0.3236406...   \n",
       "3  [-0.5539297, 1.3073884, 0.76646256, 0.10328976...   \n",
       "4  [-0.24078937, -0.09390615, -0.29544133, 0.4669...   \n",
       "\n",
       "                                          embedding3  \\\n",
       "0  [-0.70134115, -0.1560492, 1.8670809, 0.0931303...   \n",
       "1  [0.009730791, 0.496809, 0.41013932, 0.1373665,...   \n",
       "2  [-0.17629763, 0.86110705, 0.8352665, 0.2713195...   \n",
       "3  [-0.20551097, 0.21470872, 1.7353882, 0.3388543...   \n",
       "4  [0.09469901, -0.7468935, 0.6064788, 0.5366701,...   \n",
       "\n",
       "                                          embedding4  \\\n",
       "0  [-0.42886475, 0.06789703, 2.4393466, 0.1563847...   \n",
       "1  [0.12894955, 1.4916507, -0.31830812, -0.232148...   \n",
       "2  [-0.28148162, 0.32501546, 1.6964121, 0.1526436...   \n",
       "3  [0.21038896, 0.10880705, 2.3038127, 0.29001892...   \n",
       "4  [0.012085825, -1.0377014, 1.0099944, 0.3805281...   \n",
       "\n",
       "                                          embedding5  \n",
       "0  [-0.14427099, 0.28605372, 1.3342835, 0.1262898...  \n",
       "1  [0.09932873, -0.13440955, 1.4096322, 0.3468186...  \n",
       "2  [-0.36929166, 1.0149219, 1.2935079, 0.23599564...  \n",
       "3  [-0.21374436, 1.316712, 0.68170404, -0.1837967...  \n",
       "4  [-0.24604101, -0.117436886, 1.4289687, -0.1175...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(f)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "269a2de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>meaning1</th>\n",
       "      <th>meaning2</th>\n",
       "      <th>meaning3</th>\n",
       "      <th>meaning4</th>\n",
       "      <th>meaning5</th>\n",
       "      <th>embedding1</th>\n",
       "      <th>embedding2</th>\n",
       "      <th>embedding3</th>\n",
       "      <th>embedding4</th>\n",
       "      <th>embedding5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aachen</td>\n",
       "      <td>a city in western germany near the dutch and b...</td>\n",
       "      <td>formerly it was charlemagnes northern capital</td>\n",
       "      <td>aixlachapelle</td>\n",
       "      <td>aachen</td>\n",
       "      <td>aken</td>\n",
       "      <td>[0.10552966, -0.2111916, -0.042547125, 0.13038...</td>\n",
       "      <td>[-0.7410908, 0.3037946, 1.0067592, -0.03756384...</td>\n",
       "      <td>[-0.70134115, -0.1560492, 1.8670809, 0.0931303...</td>\n",
       "      <td>[-0.42886475, 0.06789703, 2.4393466, 0.1563847...</td>\n",
       "      <td>[-0.14427099, 0.28605372, 1.3342835, 0.1262898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>nocturnal burrowing mammal of the grasslands o...</td>\n",
       "      <td>sole extant representative of the order tubuli...</td>\n",
       "      <td>anteater</td>\n",
       "      <td>ant bear</td>\n",
       "      <td>orycteropus afer</td>\n",
       "      <td>[-0.15250994, 0.59140694, -0.34323326, 0.12957...</td>\n",
       "      <td>[-0.04384717, 0.6120334, 0.790435, 0.06521284,...</td>\n",
       "      <td>[0.009730791, 0.496809, 0.41013932, 0.1373665,...</td>\n",
       "      <td>[0.12894955, 1.4916507, -0.31830812, -0.232148...</td>\n",
       "      <td>[0.09932873, -0.13440955, 1.4096322, 0.3468186...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaron</td>\n",
       "      <td>united states professional baseball player who...</td>\n",
       "      <td>old testament elder brother of moses and first...</td>\n",
       "      <td>henry louis aaron</td>\n",
       "      <td>aaron</td>\n",
       "      <td>hank aaron</td>\n",
       "      <td>[-0.17691979, -0.27761042, -0.4785753, 0.32844...</td>\n",
       "      <td>[0.42689332, 1.4958502, -0.3949241, -0.3236406...</td>\n",
       "      <td>[-0.17629763, 0.86110705, 0.8352665, 0.2713195...</td>\n",
       "      <td>[-0.28148162, 0.32501546, 1.6964121, 0.1526436...</td>\n",
       "      <td>[-0.36929166, 1.0149219, 1.2935079, 0.23599564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aas</td>\n",
       "      <td>an associate degree in applied science</td>\n",
       "      <td>a dry form of lava resembling clinkers an inte...</td>\n",
       "      <td>associate in arts</td>\n",
       "      <td>aa</td>\n",
       "      <td>associate in applied science alcoholics anonym...</td>\n",
       "      <td>[-0.1955431, 0.28204358, 1.6014135, 0.2948678,...</td>\n",
       "      <td>[-0.5539297, 1.3073884, 0.76646256, 0.10328976...</td>\n",
       "      <td>[-0.20551097, 0.21470872, 1.7353882, 0.3388543...</td>\n",
       "      <td>[0.21038896, 0.10880705, 2.3038127, 0.29001892...</td>\n",
       "      <td>[-0.21374436, 1.316712, 0.68170404, -0.1837967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abaca</td>\n",
       "      <td>a kind of hemp obtained from the abaca plant i...</td>\n",
       "      <td>philippine banana tree having leafstalks that ...</td>\n",
       "      <td>manila hemp</td>\n",
       "      <td>manilla hemp</td>\n",
       "      <td>musa textilis</td>\n",
       "      <td>[-0.08495724, -1.0941188, 0.16890968, 0.784468...</td>\n",
       "      <td>[-0.24078937, -0.09390615, -0.29544133, 0.4669...</td>\n",
       "      <td>[0.09469901, -0.7468935, 0.6064788, 0.5366701,...</td>\n",
       "      <td>[0.012085825, -1.0377014, 1.0099944, 0.3805281...</td>\n",
       "      <td>[-0.24604101, -0.117436886, 1.4289687, -0.1175...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word                                           meaning1  \\\n",
       "0    aachen  a city in western germany near the dutch and b...   \n",
       "1  aardvark  nocturnal burrowing mammal of the grasslands o...   \n",
       "2     aaron  united states professional baseball player who...   \n",
       "3       aas             an associate degree in applied science   \n",
       "4     abaca  a kind of hemp obtained from the abaca plant i...   \n",
       "\n",
       "                                            meaning2           meaning3  \\\n",
       "0      formerly it was charlemagnes northern capital      aixlachapelle   \n",
       "1  sole extant representative of the order tubuli...           anteater   \n",
       "2  old testament elder brother of moses and first...  henry louis aaron   \n",
       "3  a dry form of lava resembling clinkers an inte...  associate in arts   \n",
       "4  philippine banana tree having leafstalks that ...        manila hemp   \n",
       "\n",
       "       meaning4                                           meaning5  \\\n",
       "0        aachen                                               aken   \n",
       "1      ant bear                                   orycteropus afer   \n",
       "2         aaron                                         hank aaron   \n",
       "3            aa  associate in applied science alcoholics anonym...   \n",
       "4  manilla hemp                                      musa textilis   \n",
       "\n",
       "                                          embedding1  \\\n",
       "0  [0.10552966, -0.2111916, -0.042547125, 0.13038...   \n",
       "1  [-0.15250994, 0.59140694, -0.34323326, 0.12957...   \n",
       "2  [-0.17691979, -0.27761042, -0.4785753, 0.32844...   \n",
       "3  [-0.1955431, 0.28204358, 1.6014135, 0.2948678,...   \n",
       "4  [-0.08495724, -1.0941188, 0.16890968, 0.784468...   \n",
       "\n",
       "                                          embedding2  \\\n",
       "0  [-0.7410908, 0.3037946, 1.0067592, -0.03756384...   \n",
       "1  [-0.04384717, 0.6120334, 0.790435, 0.06521284,...   \n",
       "2  [0.42689332, 1.4958502, -0.3949241, -0.3236406...   \n",
       "3  [-0.5539297, 1.3073884, 0.76646256, 0.10328976...   \n",
       "4  [-0.24078937, -0.09390615, -0.29544133, 0.4669...   \n",
       "\n",
       "                                          embedding3  \\\n",
       "0  [-0.70134115, -0.1560492, 1.8670809, 0.0931303...   \n",
       "1  [0.009730791, 0.496809, 0.41013932, 0.1373665,...   \n",
       "2  [-0.17629763, 0.86110705, 0.8352665, 0.2713195...   \n",
       "3  [-0.20551097, 0.21470872, 1.7353882, 0.3388543...   \n",
       "4  [0.09469901, -0.7468935, 0.6064788, 0.5366701,...   \n",
       "\n",
       "                                          embedding4  \\\n",
       "0  [-0.42886475, 0.06789703, 2.4393466, 0.1563847...   \n",
       "1  [0.12894955, 1.4916507, -0.31830812, -0.232148...   \n",
       "2  [-0.28148162, 0.32501546, 1.6964121, 0.1526436...   \n",
       "3  [0.21038896, 0.10880705, 2.3038127, 0.29001892...   \n",
       "4  [0.012085825, -1.0377014, 1.0099944, 0.3805281...   \n",
       "\n",
       "                                          embedding5  \n",
       "0  [-0.14427099, 0.28605372, 1.3342835, 0.1262898...  \n",
       "1  [0.09932873, -0.13440955, 1.4096322, 0.3468186...  \n",
       "2  [-0.36929166, 1.0149219, 1.2935079, 0.23599564...  \n",
       "3  [-0.21374436, 1.316712, 0.68170404, -0.1837967...  \n",
       "4  [-0.24604101, -0.117436886, 1.4289687, -0.1175...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_data = pd.concat([df1, df2], axis=1)\n",
    "words_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a29fa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21440"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b61981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>meaning1</th>\n",
       "      <th>meaning2</th>\n",
       "      <th>meaning3</th>\n",
       "      <th>meaning4</th>\n",
       "      <th>meaning5</th>\n",
       "      <th>embedding1</th>\n",
       "      <th>embedding2</th>\n",
       "      <th>embedding3</th>\n",
       "      <th>embedding4</th>\n",
       "      <th>embedding5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaron</td>\n",
       "      <td>united states professional baseball player who...</td>\n",
       "      <td>old testament elder brother of moses and first...</td>\n",
       "      <td>henry louis aaron</td>\n",
       "      <td>aaron</td>\n",
       "      <td>hank aaron</td>\n",
       "      <td>[-0.17691979, -0.27761042, -0.4785753, 0.32844...</td>\n",
       "      <td>[0.42689332, 1.4958502, -0.3949241, -0.3236406...</td>\n",
       "      <td>[-0.17629763, 0.86110705, 0.8352665, 0.2713195...</td>\n",
       "      <td>[-0.28148162, 0.32501546, 1.6964121, 0.1526436...</td>\n",
       "      <td>[-0.36929166, 1.0149219, 1.2935079, 0.23599564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abaca</td>\n",
       "      <td>a kind of hemp obtained from the abaca plant i...</td>\n",
       "      <td>philippine banana tree having leafstalks that ...</td>\n",
       "      <td>manila hemp</td>\n",
       "      <td>manilla hemp</td>\n",
       "      <td>musa textilis</td>\n",
       "      <td>[-0.08495724, -1.0941188, 0.16890968, 0.784468...</td>\n",
       "      <td>[-0.24078937, -0.09390615, -0.29544133, 0.4669...</td>\n",
       "      <td>[0.09469901, -0.7468935, 0.6064788, 0.5366701,...</td>\n",
       "      <td>[0.012085825, -1.0377014, 1.0099944, 0.3805281...</td>\n",
       "      <td>[-0.24604101, -0.117436886, 1.4289687, -0.1175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abase</td>\n",
       "      <td>cause to feel shame</td>\n",
       "      <td>hurt the pride of</td>\n",
       "      <td>humble</td>\n",
       "      <td>mortify</td>\n",
       "      <td>chagrin humiliate</td>\n",
       "      <td>[-0.06568777, -0.018044014, 1.3753768, 0.29626...</td>\n",
       "      <td>[0.07058689, -0.023161145, 1.4258491, 0.644862...</td>\n",
       "      <td>[-0.03826209, -0.22021341, 1.4970418, 0.335306...</td>\n",
       "      <td>[0.50851375, 0.36827028, 2.4313066, 0.3968379,...</td>\n",
       "      <td>[0.020256568, 0.23802361, 1.9627985, 0.5381185...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abate</td>\n",
       "      <td>make less active or intense</td>\n",
       "      <td>become less in amount or intensity</td>\n",
       "      <td>let up</td>\n",
       "      <td>slack off</td>\n",
       "      <td>die away slake slack</td>\n",
       "      <td>[0.30710104, -0.19726573, 1.3976676, 0.1641309...</td>\n",
       "      <td>[0.18145579, -0.26938137, 1.3242697, 0.3711263...</td>\n",
       "      <td>[0.17421165, -0.14604211, 2.3641486, 0.2870952...</td>\n",
       "      <td>[0.17828284, -0.14433196, 2.1683278, 0.2864399...</td>\n",
       "      <td>[0.14779839, 0.17784989, 1.7109579, 0.3073285,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abele</td>\n",
       "      <td>a poplar that is widely cultivated in the unit...</td>\n",
       "      <td>has white bark and leaves with whitish undersu...</td>\n",
       "      <td>white aspen</td>\n",
       "      <td>white poplar</td>\n",
       "      <td>silverleaved poplar populus alba aspen poplar</td>\n",
       "      <td>[-0.28837916, -0.18204403, 0.3476545, -0.10453...</td>\n",
       "      <td>[0.086071245, -0.24445783, -0.64354473, 0.1595...</td>\n",
       "      <td>[-0.6212785, 0.024624296, -0.50231385, 0.31722...</td>\n",
       "      <td>[-0.22014526, -0.23819354, -0.33803833, 0.1581...</td>\n",
       "      <td>[-0.10784195, 0.50255656, -0.11357013, 0.08043...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word                                           meaning1  \\\n",
       "0  aaron  united states professional baseball player who...   \n",
       "1  abaca  a kind of hemp obtained from the abaca plant i...   \n",
       "2  abase                                cause to feel shame   \n",
       "3  abate                        make less active or intense   \n",
       "4  abele  a poplar that is widely cultivated in the unit...   \n",
       "\n",
       "                                            meaning2           meaning3  \\\n",
       "0  old testament elder brother of moses and first...  henry louis aaron   \n",
       "1  philippine banana tree having leafstalks that ...        manila hemp   \n",
       "2                                  hurt the pride of             humble   \n",
       "3                 become less in amount or intensity             let up   \n",
       "4  has white bark and leaves with whitish undersu...        white aspen   \n",
       "\n",
       "       meaning4                                       meaning5  \\\n",
       "0         aaron                                     hank aaron   \n",
       "1  manilla hemp                                  musa textilis   \n",
       "2       mortify                              chagrin humiliate   \n",
       "3     slack off                           die away slake slack   \n",
       "4  white poplar  silverleaved poplar populus alba aspen poplar   \n",
       "\n",
       "                                          embedding1  \\\n",
       "0  [-0.17691979, -0.27761042, -0.4785753, 0.32844...   \n",
       "1  [-0.08495724, -1.0941188, 0.16890968, 0.784468...   \n",
       "2  [-0.06568777, -0.018044014, 1.3753768, 0.29626...   \n",
       "3  [0.30710104, -0.19726573, 1.3976676, 0.1641309...   \n",
       "4  [-0.28837916, -0.18204403, 0.3476545, -0.10453...   \n",
       "\n",
       "                                          embedding2  \\\n",
       "0  [0.42689332, 1.4958502, -0.3949241, -0.3236406...   \n",
       "1  [-0.24078937, -0.09390615, -0.29544133, 0.4669...   \n",
       "2  [0.07058689, -0.023161145, 1.4258491, 0.644862...   \n",
       "3  [0.18145579, -0.26938137, 1.3242697, 0.3711263...   \n",
       "4  [0.086071245, -0.24445783, -0.64354473, 0.1595...   \n",
       "\n",
       "                                          embedding3  \\\n",
       "0  [-0.17629763, 0.86110705, 0.8352665, 0.2713195...   \n",
       "1  [0.09469901, -0.7468935, 0.6064788, 0.5366701,...   \n",
       "2  [-0.03826209, -0.22021341, 1.4970418, 0.335306...   \n",
       "3  [0.17421165, -0.14604211, 2.3641486, 0.2870952...   \n",
       "4  [-0.6212785, 0.024624296, -0.50231385, 0.31722...   \n",
       "\n",
       "                                          embedding4  \\\n",
       "0  [-0.28148162, 0.32501546, 1.6964121, 0.1526436...   \n",
       "1  [0.012085825, -1.0377014, 1.0099944, 0.3805281...   \n",
       "2  [0.50851375, 0.36827028, 2.4313066, 0.3968379,...   \n",
       "3  [0.17828284, -0.14433196, 2.1683278, 0.2864399...   \n",
       "4  [-0.22014526, -0.23819354, -0.33803833, 0.1581...   \n",
       "\n",
       "                                          embedding5  \n",
       "0  [-0.36929166, 1.0149219, 1.2935079, 0.23599564...  \n",
       "1  [-0.24604101, -0.117436886, 1.4289687, -0.1175...  \n",
       "2  [0.020256568, 0.23802361, 1.9627985, 0.5381185...  \n",
       "3  [0.14779839, 0.17784989, 1.7109579, 0.3073285,...  \n",
       "4  [-0.10784195, 0.50255656, -0.11357013, 0.08043...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only keep those rows where the length of the word is 5\n",
    "words_data2 = words_data[words_data['word'].str.len() == 5] \n",
    "words_data2 = words_data2.dropna()\n",
    "words_data2 = words_data2.reset_index(drop=True)\n",
    "words_data2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6083135c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2174"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "135a6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random word from the dataset\n",
    "def choose_random_word(df):\n",
    "    index = random.randint(0,len(df))\n",
    "    print(\"index : \",index)\n",
    "    print(\"word chosen : \",words_data2[\"word\"][index])\n",
    "    return df[\"word\"][index],index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b84782e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate score based on the number of tries\n",
    "def calculate_score(tries):\n",
    "    return 15 - (tries - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e0952",
   "metadata": {},
   "source": [
    "### HINT 1 : SEMANTICALLY SIMILAR WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bc3a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d500778",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b313247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a46f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity_column3(query_vec,df):\n",
    "    l1 = []\n",
    "    match3 = {}\n",
    "    for index,sent in enumerate(df['embedding3']):\n",
    "        sim = cosine(query_vec, sent)\n",
    "\n",
    "        if sim>0.75:\n",
    "            match3[index]=sim\n",
    "            \n",
    "    print(len(match3))\n",
    "\n",
    "    s_match3 = sorted(match3.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_10_keys3 = [item[0] for item in s_match3[:10]]\n",
    "    print(top_10_keys3)\n",
    "    \n",
    "    for i in top_10_keys3:\n",
    "        print(df['word'][i])\n",
    "        l1.append(df['word'][i])\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cda2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity_column4(query_vec,df):\n",
    "    l2 = []\n",
    "    match4 = {}\n",
    "    for index,sent in enumerate(df['embedding4']):\n",
    "        sim = cosine(query_vec, sent)\n",
    "        #print(\"Sent = \", sent, \"; similarity = \", sim)\n",
    "\n",
    "        if sim>0.75:\n",
    "            match4[index]=sim\n",
    "\n",
    "    print(len(match4))\n",
    "\n",
    "    s_match4 = sorted(match4.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_10_keys4 = [item[0] for item in s_match4[:10]]\n",
    "    print(top_10_keys4)\n",
    "    \n",
    "    for i in top_10_keys4:\n",
    "        print(df['word'][i])\n",
    "        l2.append(df['word'][i])\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46f468bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity_column5(query_vec,df):\n",
    "    l3 = []\n",
    "    match5 = {}\n",
    "    for index,sent in enumerate(df['embedding5']):\n",
    "        sim = cosine(query_vec, sent)\n",
    "        #print(\"Sent = \", sent, \"; similarity = \", sim)\n",
    "\n",
    "        if sim>0.75:\n",
    "            match5[index]=sim\n",
    "    print(len(match5))\n",
    "\n",
    "    s_match5 = sorted(match5.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_10_keys5 = [item[0] for item in s_match5[:10]]\n",
    "    print(top_10_keys5)\n",
    "    \n",
    "    for i in top_10_keys5:\n",
    "        print(df['word'][i])\n",
    "        l3.append(df['word'][i])\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1947beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Find a word semantically similar to the chosen word\n",
    "def find_similar_word(query, df):\n",
    "    query_vec = model1.encode([query])[0]\n",
    "    l1 = semantic_similarity_column3(query_vec,words_data)\n",
    "    l2 = semantic_similarity_column4(query_vec,words_data)\n",
    "    l3 = semantic_similarity_column5(query_vec,words_data)\n",
    "    \n",
    "    final_list = l1+l2+l3\n",
    "    final_list = sorted(list(set(final_list)))\n",
    "    \n",
    "    if query in final_list:\n",
    "        final_list.remove(query)\n",
    "    print(final_list)\n",
    "    \n",
    "    length = len(final_list)\n",
    "    list_for_hint = []\n",
    "    for i in range(5):\n",
    "        random_index = random.randint(0,length-1)\n",
    "        if final_list[random_index] not in list_for_hint:\n",
    "            list_for_hint.append(final_list[random_index])\n",
    "    print(list_for_hint)\n",
    "    return list_for_hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac07195",
   "metadata": {},
   "source": [
    "### HINT 2 : KEYWORD EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53e3fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyphrase_vectorizers import KeyphraseCountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5544d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b959f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3eec18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 2: Keyword extraction from the meanings\n",
    "def extract_keywords(kw_model,random_word):\n",
    "    index = words_data[words_data['word'] == random_word].index.tolist()\n",
    "    meaning1 = words_data[\"meaning1\"][index]\n",
    "    meaning2 = words_data[\"meaning2\"][index]\n",
    "\n",
    "    # Combine meanings for keyphrase extraction\n",
    "    combined_text = meaning1 + \" \" + meaning2\n",
    "    combined_text = str(combined_text)\n",
    "\n",
    "    # Extract keyphrases\n",
    "    keyphrases = kw_model.extract_keywords(docs=[combined_text], \n",
    "                                         vectorizer=KeyphraseCountVectorizer())\n",
    "\n",
    "    filtered_data = [item for item in keyphrases if item[0] not in ('dtype', 'object')]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940177d5",
   "metadata": {},
   "source": [
    "### HINT 3: CLUSTER EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee0112e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fac3e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "words = words_data['word'].values.tolist()\n",
    "synonyms = words_data.drop(columns=['word','meaning1','meaning2','meaning5',\n",
    "                                    'embedding1','embedding2','embedding3',\n",
    "                                   'embedding4','embedding5']).values.flatten().tolist()\n",
    "unique_synonyms = list(set(synonyms))\n",
    "all_words = words + unique_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d5b294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d86d44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_embeddings = distilbert_model.encode(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1d7b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to perform clustering using DistilBERT embeddings\n",
    "def perform_clustering(embeddings, n_clusters=3):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    return kmeans, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bf87352",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_distil, clusters_distil = perform_clustering(distil_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9293968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to find similar words in clusters based on chosen word embedding\n",
    "def find_similar_words_in_clusters(embeddings, cluster_labels, chosen_word_embedding, all_words, k=10):\n",
    "    similar_words = []\n",
    "    for cluster_id in range(cluster_labels.max() + 1):\n",
    "        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "        cluster_embeddings = embeddings[cluster_indices]\n",
    "        cluster_words = np.array(all_words)[cluster_indices]\n",
    "        distances = np.linalg.norm(cluster_embeddings - chosen_word_embedding, axis=1)\n",
    "        closest_indices = np.argsort(distances)[:k]\n",
    "        similar_words.extend(cluster_words[closest_indices])\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68533b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cae0f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordle_game(words_data, words_data2,kw_model, distil_model, distil_embeddings, kmeans_distil, clusters_distil, all_words):\n",
    "    chosen_word, chosen_index = choose_random_word(words_data2)\n",
    "    tries = 0\n",
    "    score = 15\n",
    "    hint_used = {'semantic': False, 'keyword': False, 'clustering': False}\n",
    "\n",
    "    print(\"Welcome to Wordle! You have 7 tries to guess the word.\")\n",
    "\n",
    "    while tries < 7:\n",
    "        guess = input(f\"\\nAttempt {tries + 1}: Enter your guess (or type 'give up' to give up): \").lower()\n",
    "\n",
    "        if guess == 'give up':\n",
    "            print(f\"Sorry, you've given up. The word was '{chosen_word}'!\")\n",
    "            score -= 8\n",
    "            break\n",
    "\n",
    "        if guess == chosen_word.lower():\n",
    "            print(f\"Congratulations! You've guessed the word '{chosen_word}' correctly!\")\n",
    "            score += 10\n",
    "            break\n",
    "\n",
    "        score -= 2\n",
    "\n",
    "        correct_letters = sum(1 for x, y in zip(guess, chosen_word) if x == y)\n",
    "        misplaced_letters = sum(min(guess.count(letter), chosen_word.count(letter)) for letter in set(guess)) - correct_letters\n",
    "\n",
    "        correct_letters = 0\n",
    "        misplaced_letters = 0\n",
    "        correct_positions = []\n",
    "        correct_letters_wrong_position = []\n",
    "\n",
    "        # Check for correct letters in correct positions and correct letters in wrong positions\n",
    "        for idx, letter in enumerate(guess):\n",
    "            if letter == chosen_word[idx]:\n",
    "                correct_letters += 1\n",
    "                correct_positions.append(letter)\n",
    "            elif letter in chosen_word:\n",
    "                misplaced_letters += 1\n",
    "                correct_letters_wrong_position.append(letter)\n",
    "\n",
    "        print(f\"Correct letters in correct positions: {correct_positions}\")\n",
    "        print(f\"Correct letters in wrong positions: {correct_letters_wrong_position}\")\n",
    "        print(f\"Your current score: {score}\")\n",
    "        \n",
    "        tries += 1\n",
    "\n",
    "        if tries >= 1:\n",
    "            hint_choice = input(\"Do you want to use a hint? (yes/no): \").lower()\n",
    "            if hint_choice == 'yes':\n",
    "                if not hint_used['semantic']:\n",
    "                    hint_used['semantic'] = True\n",
    "                    similar_word = find_similar_word(chosen_word, words_data)\n",
    "                    print(f\"\\nHint 1: A word similar to the chosen word is '{similar_word}'.\")\n",
    "                elif not hint_used['keyword']:\n",
    "                    hint_used['keyword'] = True\n",
    "                    print(\"Hint 2: Keywords extracted from the meanings are:\", extract_keywords(kw_model,chosen_word))\n",
    "                elif not hint_used['clustering']:\n",
    "                    hint_used['clustering'] = True\n",
    "                    chosen_word_embedding = distil_model.encode([chosen_word])[0]\n",
    "                    similar_words = find_similar_words_in_clusters(distil_embeddings, clusters_distil, \n",
    "                                                                   chosen_word_embedding, all_words)\n",
    "                    clustering_words=[]\n",
    "                    for i in range(5):\n",
    "                        for j in similar_words:\n",
    "                            if j!=chosen_word and j not in clustering_words:\n",
    "                                clustering_words.append(j)\n",
    "                    print(f\"\\nHint 3: Words clustered with based on embeddings are:\", clustering_words)\n",
    "                else:\n",
    "                    print(\"You have already used all available hints.\")\n",
    "            else:\n",
    "                print(\"No hint used.\")\n",
    "\n",
    "    if tries >= 7:\n",
    "        print(f\"\\nSorry, you've run out of tries! The word was '{chosen_word}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc54d74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index :  1927\n",
      "word chosen :  thumb\n",
      "Welcome to Wordle! You have 7 tries to guess the word.\n",
      "\n",
      "Attempt 1: Enter your guess (or type 'give up' to give up): think\n",
      "Correct letters in correct positions: ['t', 'h']\n",
      "Correct letters in wrong positions: []\n",
      "Your current score: 13\n",
      "Do you want to use a hint? (yes/no): yes\n",
      "1592\n",
      "[7300, 12306, 7363, 9427, 13956, 13962, 14014, 19403, 13653, 19402]\n",
      "finis\n",
      "moved\n",
      "flagellation\n",
      "immoral\n",
      "pinnacle\n",
      "pinwheel\n",
      "pixy\n",
      "travail\n",
      "pelecypod\n",
      "trauma\n",
      "1620\n",
      "[7298, 7300, 16289, 8529, 8530, 16819, 9426, 9428, 9783, 12118]\n",
      "fingerroot\n",
      "finis\n",
      "satiate\n",
      "guilder\n",
      "guile\n",
      "sharpness\n",
      "immodest\n",
      "immorality\n",
      "inflexible\n",
      "mogadiscio\n",
      "240\n",
      "[13588, 8659, 4306, 19216, 3541, 12697, 11143, 19570, 5816, 14655]\n",
      "payable\n",
      "hangdog\n",
      "couch\n",
      "tool\n",
      "clucking\n",
      "noisy\n",
      "local\n",
      "tucker\n",
      "doting\n",
      "propel\n",
      "['clucking', 'couch', 'doting', 'fingerroot', 'finis', 'flagellation', 'guilder', 'guile', 'hangdog', 'immodest', 'immoral', 'immorality', 'inflexible', 'local', 'mogadiscio', 'moved', 'noisy', 'payable', 'pelecypod', 'pinnacle', 'pinwheel', 'pixy', 'propel', 'satiate', 'sharpness', 'tool', 'trauma', 'travail', 'tucker']\n",
      "['immodest', 'immorality', 'tucker', 'local']\n",
      "\n",
      "Hint 1: A word similar to the chosen word is '['immodest', 'immorality', 'tucker', 'local']'.\n",
      "\n",
      "Attempt 2: Enter your guess (or type 'give up' to give up): thoop\n",
      "Correct letters in correct positions: ['t', 'h']\n",
      "Correct letters in wrong positions: []\n",
      "Your current score: 11\n",
      "Do you want to use a hint? (yes/no): yes\n",
      "Hint 2: Keywords extracted from the meanings are: [('thick short innermost digit', 0.6227), ('forelim', 0.3705)]\n",
      "\n",
      "Attempt 3: Enter your guess (or type 'give up' to give up): thamp\n",
      "Correct letters in correct positions: ['t', 'h', 'm']\n",
      "Correct letters in wrong positions: []\n",
      "Your current score: 9\n",
      "Do you want to use a hint? (yes/no): yes\n",
      "\n",
      "Hint 3: Words clustered with based on embeddings are: ['nail down', 'rub down', 'down', 'partial tone', 'decease', 'hunker down', 'write down', 'secern', 'slow down', 'rule of thumb', 'finger spelling', 'palmshaped', 'zib', 'briny', 'chapman', 'lipread', 'palmyra', 'thumbed', 'finger', 'palm', 'hand', 'fingered', 'lip']\n",
      "\n",
      "Attempt 4: Enter your guess (or type 'give up' to give up): give up\n",
      "Sorry, you've given up. The word was 'thumb'!\n"
     ]
    }
   ],
   "source": [
    "wordle_game(words_data,words_data2,kw_model,distilbert_model, distil_embeddings, kmeans_distil, clusters_distil, all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933c3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
